---
title: Monitoring PCF Healthwatch
owner: PCF Healthwatch
---

This topic explains how to monitor the health of Pivotal Cloud Foundry (PCF) Healthwatch using the metrics and key performance indicators (KPIs) generated by the service.

For general information about monitoring PCF, see [Monitoring Pivotal Cloud Foundry](https://docs.pivotal.io/pivotalcf/monitoring/index.html).

## <a id="about-metrics"></a>About Metrics

PCF Healthwatch emits metrics in the following format:

<pre>
origin:"healthwatch" eventType:ValueMetric timestamp:1509638101820496694 deployment:"healthwatch-app-dev-v1-1" job:"healthwatch-forwarder" index:"097f4b1e-5ca8-4866-82d5-00883798dad4" ip:"10.0.16.29" valueMetric:&lt;name:"healthwatch.metrics.published" value:38 unit:"count"&gt;
</pre>

All PCF Healthwatch-emitted metrics have the `healthwatch` origin.

## <a id="keyPerformanceIndicators"></a>Key Performance Indicators for PCF Healthwatch

This section describes the KPIs that you can use to monitor the health of PCF Healthwatch.

###<a id="firehose-disconnect"></a>Number of PCF Healthwatch Nozzle Disconnects from Firehose
<table>
   <tr><th colspan="2" style="text-align: center;"><br/> healthwatch.ingestor.disconnects<br/><br/></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>
        Number of forced disconnects of the <a href="./architecture.html">PCF Healthwatch data ingestor nozzle</a> from the Firehose.<br/><br/>
        <strong>Use</strong>: An unusual increase in the number of disconnects from the Firehose typically indicates that you need to scale the nozzle up. The Firehose disconnects nozzles that are slow consumers to protect apps from backpressure. This metric can also spike during a PCF deployment because the Traffic Controller VMs restart, logging a disconnect.
        <br/><br/>
        A prolonged period of losing metrics as a result of disconnects can endanger the assessments that PCF Healthwatch makes using platform metrics from the Firehose.
        <br/><br/>
        <strong>Origin</strong>: Firehose<br/>
        <strong>Type</strong>: Gauge<br/>
        <strong>Frequency</strong>: 60 s<br/>
      </td>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Average over last 5 minutes</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td>
        <strong>Yellow warning</strong>: Dynamic<br/>
        <strong>Red critical</strong>: Dynamic
      </td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
        If no known deployment occurred and the spike is sustained, increase the number of PCF Healthwatch Ingestor instances and monitor this metric to ensure that it returns to a normal state.
        <br/><br/>
        You can scale Ingestor instances in the <strong>Healthwatch Component Config</strong> tab of the PCF Healthwatch tile or using the <code>cf scale healthwatch-ingestor</code> command. While <code>cf scale</code> helps you to quickly scale the instances, you should also update the tile configuration so that the next deployment does not override the manual scaling.
      </td>
   </tr>
</table>


###<a id="ingestor-dropped"></a>Number of Ingestor Dropped Metrics
<table>
   <tr><th colspan="2" style="text-align: center;"><br/> healthwatch.ingestor.dropped<br/><br/></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>
        Number of metrics dropped by the <a href="./architecture.html">PCF Healthwatch Ingestor</a>, which loads incoming data into the PCF Healthwatch Redis datastore.
        <br/><br/>
        <strong>Use</strong>: An unusual increase in the number of dropped messages by the PCF Healthwatch Ingestor likely indicates that you need to scale up this component and verify the health of Redis.
         A prolonged period of dropping messages can endanger the assessments that PCF Healthwatch makes using platform metrics from the Firehose.
        <br/><br/>
        <strong>Origin</strong>: healthwatch<br/>
        <strong>Type</strong>: Gauge<br/>
        <strong>Frequency</strong>: 60 s<br/>
      </td>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Average over last 5 minutes</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: Dynamic<br/>
      <strong>Red critical</strong>: Dynamic</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
        Verify the health of the Redis VM and increase the number of PCF Healthwatch Ingestor instances. Monitor this metric to ensure that it returns to a normal state.
        <br/><br/>
        You can scale Ingestor instances using the <code>cf scale healthwatch-ingestor</code> command.
        While <code>cf scale</code> helps you to quickly scale the instances, you should also update the Ingestor Count in the tile configuration located in Healthwatch Component Config tab. Otherwise, the next <code>Apply Changes</code> will override the manual scaling.
      </td>
   </tr>
</table>

###<a id="redis-queue-size"></a>Redis Queue Size
<table>
   <tr><th colspan="2" style="text-align: center;"><br/>healthwatch.redis.valueMetricQueue.size<br/>healthwatch.redis.counterEventQueue.size<br/><br/></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>
        Number of Firehose metrics that are queued for aggregation into Healthwatch.
        <br/><br/>
        <strong>Use</strong>: An unusual spike in the number of queued metrics can indicate that PCF Healthwatch Workers are unable to keep up with the volume of metrics from the Firehose. A large Redis queue will result in value metrics and counter events being delayed; if the queue becomes completely full, metrics will be lost altogether. This will also adversely affect PCF Healthwatch's ability to calculate super value metrics.
        <br/><br/>
        <strong>Origin</strong>: Firehose<br/>
        <strong>Type</strong>: Gauge<br/>
        <strong>Frequency</strong>: 60 s<br/>
      </td>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Average over last 5 minutes</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Red critical</strong>: &ge;10,000</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
        If the spike is sustained, increase the number of PCF Healthwatch Worker instances and monitor this metric to ensure that it returns to a normal state.
        <br/><br/>
        You can scale Worker instances in the <strong>Healthwatch Component Config</strong> tab of the PCF Healthwatch tile or using the <code>cf scale healthwatch-worker</code> command. While <code>cf scale</code> helps you to quickly scale the instances, you should also update the tile configuration so that the next deployment does not override the manual scaling.
      </td>
   </tr>
</table>

###<a id="ui-availability"></a>PCF Healthwatch UI Availability
<table>
   <tr><th colspan="2" style="text-align: center;"><br/> healthwatch.ui.available<br/><br/></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>
        The PCF Healthwatch UI is currently available. This assessment is made using a probe that looks for a successful response: 1 = available, 0 = not available, or timeout (10 s).
        <br/><br/>
        <strong>Use</strong>: Indicates that the Healthwatch UI is running and available to product users. While an issue with the UI does not impact the assessments that PCF Healthwatch is making, loss of the UI can impact user ability to visually reference these assessments.
        <br/><br/>
        <strong>Origin</strong>: Firehose<br/>
        <strong>Type</strong>: Gauge<br/>
        <strong>Frequency</strong>: 60 s<br/>
      </td>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Average over last 5 minutes</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: &le; 0.6<br/>
      <strong>Red critical</strong>: &le; 0.4</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
        <ol>
          <li>Ensure the <code>healthwatch</code> app is running in the <code>healthwatch</code> space of the <code>system</code> org.</li> 
          <li>Check the app logs for any obvious errors.</li>
          <li>Verify that the <code>/info</code> endpoint is available on the <code>healthwatch</code> app route.</li>
        </ol>
      </td>
   </tr>
</table>

###<a id="cli-test-availability"></a>CLI Health Test Availability
<table>
   <tr><th colspan="2" style="text-align: center;"><br/> health.check.cliCommand.probe.available<br/><br/></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>
        PCF Healthwatch has up-to-date results for the <a href="metrics.html#cli">CLI Command Health Test</a>, which means that the test was recently available.
        <br/><br/>
        Metric values: 1 = available, 0 = not available, or timeout (10 s)
        <br/><br/>
        This assessment is made by looking for results within the configured test schedule plus timeout. For example, a test runner scheduled on 5-minute intervals with a 2-minute timeout must show a test result within the last 7 minutes to succeed.
        <br/><br/>
        <strong>Use</strong>: Indicates that PCF Healthwatch is assessing the health of the Cloud Foundry Command Line Interface (cf CLI) commands. If these continuous validation tests fail to make up-to-date assessments, they are no longer a reliable warning mechanism.
        <br/><br/>
        <strong>Origin</strong>: Firehose<br/>
        <strong>Type</strong>: Gauge<br/>
        <strong>Frequency</strong>: 60 s<br/>
      </td>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Average over last 5 minutes</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: &le; 0.6<br/>
      <strong>Red critical</strong>: &le; 0.4</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
        <ol>
          <li>Ensure the <code>cf-health-check</code> app is running in the <code>healthwatch</code> space of the <code>system</code> org.</li> 
          <li>Check the app logs for any obvious errors.</li>
      </td>
   </tr>
</table>

###<a id="canary-test-availability"></a>Canary App Health Test Availability
<table>
   <tr><th colspan="2" style="text-align: center;"><br/> health.check.CanaryApp.probe.available<br/><br/></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>
        PCF Healthwatch has up-to-date results for the <a href="metrics.html#canaryapp">Canary App Health Test</a>, which means that the test was recently available.
        <br/><br/>
        Metric values: 1 = available, 0 = not available, or timeout (10 s)
        <br/><br/>
        This assessment of up-to-date results is made by looking for results within the configured test schedule plus timeout. For example, a test runner scheduled on 5-minute intervals with a 2-minute timeout must show a test result within the last 7 minutes to succeed.
        <br/><br/>
        <strong>Use</strong>: Indicates that PCF Healthwatch is assessing the current state of health for the canary app. If this continuous validation test fails to make up-to-date assessments, it is no longer a reliable warning mechanism.
        <br/><br/>
        <strong>Origin</strong>: Firehose<br/>
        <strong>Type</strong>: Gauge<br/>
        <strong>Frequency</strong>: 60 s<br/>
      </td>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Average over last 5 minutes</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: &le; 0.6<br/>
      <strong>Red critical</strong>: &le; 0.4</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
        <ol>
          <li>Ensure the <code>canary-health-check</code> app is running in the <code>healthwatch</code> space of the <code>system</code> org. Check the app logs for any obvious errors.</li>
          <li>Verify that Apps Manager is running and accessible through the URL configured in the <code>CANARY_URL</code> environment variable of the <code>canary-health-check</code> app.</li>
        </ol>
      </td>
   </tr>
</table>

###<a id="bosh-test-availability"></a>BOSH Director Health Test Availability
<table>
   <tr><th colspan="2" style="text-align: center;"><br/> health.check.bosh.director.probe.available<br/><br/></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>
        PCF Healthwatch has up-to-date results for the <a href="metrics.html#bosh-director">BOSH Director Health Test</a>, which means that the test was recently available.
        <br/><br/>
        Metric values: 1 = available, 0 = not available, or timeout (10 s)
        <br/><br/>
        This assessment of up-to-date results is made by looking for results within the configured test schedule plus timeout. For example, a test runner scheduled on 5-minute intervals with a 2-minute timeout must show a test result within the last 7 minutes to succeed.
        <br/><br/>
        <strong>Use</strong>: Indicates that PCF Healthwatch is assessing the current state of health for the BOSH Director. If this continuous validation test fails to make up-to-date assessments, it is no longer a reliable warning mechanism.
        <br/><br/>
        <strong>Origin</strong>: Firehose<br/>
        <strong>Type</strong>: Gauge<br/>
        <strong>Frequency</strong>: 60 s<br/>
      </td>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Average over last 5 minutes</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: &le; 0.6<br/>
      <strong>Red critical</strong>: &le; 0.4</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
        <ol>
          <li>Ensure the <code>bosh-health-check</code> app is running in the <code>healthwatch</code> space of the <code>system</code> org. Check the app logs for any obvious errors.</li>
          <li>SSH into the running <code>bosh-health-check</code> app and copy the BOSH manifest from <code>/home/vcap/app/health_check_manifest.yml</code>. Try to deploy it manually on the BOSH Director and check for errors.</li>
        </ol>
      </td>
   </tr>
</table>

###<a id="opsman-test-availability"></a>Ops Manager Health Test Availability
<table>
   <tr><th colspan="2" style="text-align: center;"><br/> health.check.OpsMan.probe.available<br/><br/></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>
        PCF Healthwatch has up-to-date results for the <a href="metrics.html#opsman">Ops Manager Health Test</a>, which means that the test was recently available.
        <br/><br/>
        Metric values: 1 = available, 0 = not available, or timeout (10 s)
        <br/><br/>
        This assessment of up-to-date results is made by looking for results within the configured test schedule plus timeout. For example, a test runner scheduled on 5-minute intervals with a 2-minute timeout must show a test result within the last 7 minutes to succeed.
        <br/><br/>
        <strong>Use</strong>: Indicates that PCF Healthwatch is assessing the current state of health for Ops Manager. If this continuous validation test fails to make up-to-date assessments, it is no longer a reliable warning mechanism.
        <br/><br/>
        <strong>Origin</strong>: Firehose<br/>
        <strong>Type</strong>: Gauge<br/>
        <strong>Frequency</strong>: 60 s<br/>
      </td>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Average over last 5 minutes</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: &le; 0.6<br/>
      <strong>Red critical</strong>: &le; 0.4</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
          <ol>
            <li>Ensure the <code>opsmanager-health-check</code> app is running in the <code>healthwatch</code> space of the <code>system</code> org. Check the app logs for any obvious errors.</li>
            <li>Verify that Ops Manager is running and accessible through the URL configured in the <code>OPSMANAGER_URL</code> environment variable of the <code>opsmanager-health-check</code> app.</li>
          </ol>
      </td>
   </tr>
</table>

###<a id="super-metrics-published"></a>Number of Healthwatch Super Metrics Published to Firehose
<table>
   <tr><th colspan="2" style="text-align: center;"><br/>healthwatch.metrics.published<br/><br/></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>
        Number of <a href="metrics.html">PCF Healthwatch Metrics</a> published back to the Firehose.
        <br/><br/>
        <strong>Use</strong>: If an operator has not made changes that impact the number or frequency of assessments, an unusual drop in the number of metrics published can indicate that PCF Healthwatch may be experiencing a computation or publication issue.
        <br/><br/>
        <strong>Origin</strong>: Firehose<br/>
        <strong>Type</strong>: Gauge<br/>
        <strong>Frequency</strong>: 60 s<br/>
      </td>
   </tr>
   <tr>
      <th>Recommended measurement</th>
      <td>Average over last 5 minutes</td>
   </tr>
   <tr>
      <th>Recommended alert thresholds</th>
      <td><strong>Yellow warning</strong>: Dynamic<br/>
      <strong>Red critical</strong>: Dynamic</td>
   </tr>
   <tr>
      <th>Recommended response</th>
      <td>
          <ol>
            <li>Verify that the <code>healthwatch-forwarder</code> VM is running.</li>
            <li>Check all of the logs in <code>/var/vcap/sys/log</code> on the VM.</li>
            <li>Verify that the <code>*-health-check</code> apps are running and the logs in the <code>healthwatch</code> space of the <code>system</code> org are not receiving any obvious errors from them.</li>
          </ol>
      </td>
   </tr>
</table>

##<a id="continuous-validation-tests"></a>Number of Healthwatch Continuous Validation Tests Executed

This section describes the metrics that you can use to monitor the number of continuous validation tests executed by PCF Healthwatch.

###<a id="cli-command-health-probe"></a>CLI Command Health
<table>
   <tr><th colspan="2" style="text-align: center;"><br/> health.check.cliCommand.probe.count<br/><br/></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>
        Number of PCF Healthwatch <a href="metrics.html#cli">CLI Command Health</a> probe assessments completed in the measured time interval.
        <br/><br/>
        <strong>Use</strong>: For alerting purposes, Pivotal suggests using <code>health.check.cliCommand.probe.available</code> instead. This metric is most helpful for additional diagnostics or secondary alerting.
        <br/><br/>
        When monitoring this metric, the primary indicator of concern is an unexpected negative variance from the normal pattern of checks per test type. If an operator has not made changes that impact the number of checks being made, such as scaling the test runner or changing the frequency of the test, an unexpected variance from normal likely indicates problems in the test runner functionality.
        <br/><br/>
        In the default installation, these tests run every 5 minutes across 2 runner apps.
        <br/><br/>
        <strong>Origin</strong>: Firehose<br/>
        <strong>Type</strong>: Gauge<br/>
        <strong>Frequency</strong>: 60 s<br/>
      </td>
   </tr>
</table>

###<a id="opsmanager-health-probe"></a>Ops Manager Health
<table>
   <tr><th colspan="2" style="text-align: center;"><br/> health.check.OpsMan.probe.count<br/><br/></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>
        Number of PCF Healthwatch <a href="metrics.html#opsman">Ops Manager Health</a> probe assessments completed in the measured time interval.
        <br/><br/>
        <strong>Use</strong>: For alerting purposes, Pivotal suggests using <code>health.check.OpsMan.probe.available</code> instead. This metric is most helpful for additional diagnostics or secondary alerting.
        <br/><br/>
        When monitoring this metric, the primary indicator of concern is an unexpected negative variance from the normal pattern of checks per test type. If an operator has not made changes that impact the number of checks being made, such as scaling the test runner or changing the frequency of the test, an unexpected variance from normal likely indicates problems in the test runner functionality.
        <br/><br/>
        In the default installation, these tests run every 1 minute across 2 runner apps.
        <br/><br/>
        <strong>Origin</strong>: Firehose<br/>
        <strong>Type</strong>: Gauge<br/>
        <strong>Frequency</strong>: 60 s<br/>
      </td>
   </tr>
</table>

###<a id="canary-health-probe"></a>Canary App Health
<table>
   <tr><th colspan="2" style="text-align: center;"><br/> health.check.CanaryApp.probe.count<br/><br/></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>
        Number of PCF Healthwatch <a href="metrics.html#canaryapp">Canary App Health</a> probe assessments completed in the measured time interval.
        <br/><br/>
        <strong>Use</strong>: For alerting purposes, Pivotal suggests using <code>health.check.CanaryApp.probe.available</code> instead. This metric is most helpful for additional diagnostics or secondary alerting.
        <br/><br/>
        When monitoring this metric, the primary indicator of concern is an unexpected negative variance from the normal pattern of checks per test type. If an operator has not made changes that impact the number of checks being made, such as scaling the test runner or changing the frequency of the test, an unexpected variance from normal likely indicates problems in the test runner functionality.
        <br/><br/>
        In the default installation, these tests run every 1 minutes across 2 runner apps.
        <br/><br/>
        <strong>Origin</strong>: Firehose<br/>
        <strong>Type</strong>: Gauge<br/>
        <strong>Frequency</strong>: 60 s<br/>
      </td>
   </tr>
</table>

###<a id="bosh-health-probe"></a>BOSH Director Health
<table>
   <tr><th colspan="2" style="text-align: center;"><br/> health.check.bosh.director.probe.count<br/><br/></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>
        Number of PCF Healthwatch <a href="metrics.html#bosh-director">BOSH Director Health</a> probe assessments completed in the measured time interval.
        <br/><br/>
        <strong>Use</strong>: For alerting purposes, Pivotal suggests using <code>health.check.bosh.director.probe.available</code> instead. This metric is most helpful for additional diagnostics or secondary alerting.
        <br/><br/>
        When monitoring this metric, the primary indicator of concern is an unexpected negative variance from the normal pattern of checks per test type. If an operator has not made changes that impact the number of checks being made, such as scaling the test runner or changing the frequency of the test, an unexpected variance from normal likely indicates problems in the test runner functionality.
        <br/><br/>
        In the default installation, these tests run every 10 minutes using 1 runner app.
        <br/><br/>
        <strong>Origin</strong>: Firehose<br/>
        <strong>Type</strong>: Gauge<br/>
        <strong>Frequency</strong>: 60 s<br/>
      </td>
   </tr>
</table>

##<a id="other-metrics"></a>Other Metrics Available

This section describes other metrics that you can use to monitor PCF Healthwatch.

###<a id="healthwatch-events-published"></a>Number of Healthwatch Events Published to PCF Event Alerts
<table>
   <tr><th colspan="2" style="text-align: center;"><br/> healthwatch.events.published<br/><br/></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>
        Number of PCF Healthwatch Event Alerts triggered and published to <a href="http://docs.pivotal.io/event-alerts/index.html">PCF Event Alerts</a>.
        <br/><br/>
        <strong>Use</strong>: This metric is primarily interesting for informational purposes. As the number of alerting events could vary greatly, it is not recommended to alert on this metric itself.
        <br/><br/>
        <strong>Origin</strong>: Firehose<br/>
        <strong>Type</strong>: Gauge<br/>
        <strong>Frequency</strong>: 60 s<br/>
      </td>
   </tr>
</table>

###<a id="bosh-deployment-probe"></a>BOSH Deployment Check Probe
<table>
   <tr><th colspan="2" style="text-align: center;"><br/> health.bosh.deployment.probe.count<br/><br/></th></tr>
   <tr>
      <th width="25%">Description</th>
      <td>
        Number of PCF Healthwatch <a href="metrics.html#bosh-deployment">BOSH Deployment Occurrence</a> probes completed in the measured time interval.
        <br/><br/>
        <strong>Use</strong>: When monitoring this metric, the primary indicator of concern is an unexpected negative variance from the normal pattern of checks per test type. If an operator has not made changes that impact the number of checks being made, such as scaling the test runner or changing the frequency of the test, an unexpected variance from normal likely indicates problems in the test runner functionality.
        <br/><br/>
        In the default installation, these tests run every 30 seconds across 2 runner apps.
        <br/><br/>
        <strong>Origin</strong>: Firehose<br/>
        <strong>Type</strong>: Gauge<br/>
        <strong>Frequency</strong>: 60 s<br/>
      </td>
   </tr>
</table>

