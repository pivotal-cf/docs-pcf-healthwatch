---
title: Installing Healthwatch
owner: Healthwatch
---

This topic describes how to install and configure Healthwatch. 

## Install Healthwatch

1. Download the product file from [Healthwatch](https://network.pivotal.io/products/p-healthwatch/).

1. Navigate to the Ops Manager Installation Dashboard and click **Import a Product** to upload the product file.

1. Under the **Import a Product** button, click **+** next to the version number of Healthwatch.
This adds the tile to your staging area.

<p class="note"><strong>Note:</strong> Installing using Platform Automation 

    The following config file for the <code>download-product</code> task will work to fetch the Healthwatch tile (without getting the exporters):
</p>
```yaml
---
pivnet-api-token: token
pivnet-file-glob: "Healthwatch-view-[^pks|pas]*.pivotal"
pivnet-product-slug: Healthwatch_view_pcf
product-version-regex: "0.4.*"
```
<p class="note"><strong>Note:</strong> When Healthwatch PAS Exporter or Healthwatch PKS Exporter on the same foundation as
Healthwatch, there is no need to manually configure a scrape job for them. These exporters are automatically discovered and scraped.
This section is for adding additional custom scrape jobs or to scrape exporters installed on different foundations.</p>

##  Configure the Healthwatch Tile

To configure the Healthwatch tile, perform the following steps:

1. Click the **Healthwatch** tile on the Ops Manager Installation Dashboard.
1. Navigate to **Assign AZs and Networks** and do the following:
    1. Select an Availability Zone (AZ) for placing singleton jobs.
    1. Select one or more AZs for balancing other jobs.
    1. Select **Network** for installing Healthwatch.
    1. Click **Save**.

1. Navigate to **Time series database (TSDB) Configuration** and do the following:
    1. Optional: Modify the **Scrape Interval** as desired. This controls the frequency at which the TSDB scrapes its targets for metrics. Healthwatch recommends using a 15 seconds scraping interval to provide balance between metric detail and storage consumption.
    1. Optional: Add additional jobs to the `Additional Scrape Config Jobs`.  This entry section allows you to add jobs to the TSDB.yml configuration
       file.
       * The first field, **TSDB Scrape job**  takes a YAML formatted job in the format specified
       by [TSDB](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config) This job can use any of the properties
       defined by TSDB itself except for the`tls_config` property.  The entries for that can be filled in below. Note that this box takes a single
       scrape job.  The job should not be prepended with the YAML array *-*. An example of the scrape config should look like:

```yaml
job_name: foundation1
metrics_path: /metrics
scheme: https
static_configs:
  - targets:
    - "1.2.3.4:9090"
    - "5.6.7.8:9090"
```
       * Optional: **TLS Config Certificate Authority** This field takes a CA Certificate that will end up in the **ca_file** property of the **tls_config**.
       * Optional: **TLS Config Certificate and Private Key** This field takes a Certificate and Private key.  These properties will end up in the **cert_file**
       and **key_file** properties of the **tls_config**
       * Optional: **TLS Config Server Name** This field takes a string server name that will end up in the **server_name** field of the **tls_config**
   1. If using `om` to configure the tile, you will want a configuration like the following:

```yaml
product-name: p-reliability-view
product-properties:
  .properties.scrape_configs:
    value:
      - ca: |
          -----BEGIN CERTIFICATE-----
          SECRET
          -----END CERTIFICATE-----
        scrape_job: |
          job_name: foundation1
          metrics_path: /metrics
          scheme: https
          static_configs:
            - targets:
              - "1.2.3.4:9090"
              - "5.6.7.8:9090"
        server_name: pasexporter
        tls_certificates:
          cert_pem: |
            -----BEGIN CERTIFICATE-----
            SECRET
            -----END CERTIFICATE-----
          private_key_pem: |
            -----BEGIN RSA PRIVATE KEY-----
            SECRET
            -----END RSA PRIVATE KEY-----
      - ca: |
          -----BEGIN CERTIFICATE-----
          SECRET
          -----END CERTIFICATE-----
        scrape_job: |
          job_name: foundation2
          metrics_path: /metrics
          scheme: https
          static_configs:
            - targets:
              - "9.10.11.12:9090"
        server_name: pasexporter
        tls_certificates:
          cert_pem: |
            -----BEGIN CERTIFICATE-----
            SECRET
            -----END CERTIFICATE-----
          private_key_pem: |
            -----BEGIN RSA PRIVATE KEY-----
            SECRET
            -----END RSA PRIVATE KEY-----
  .properties.ui_authentication:
    selected_option: basic
    value: basic
  .tsdb.blackbox_exporter_port:
    value: 9115
  .tsdb.scrape_interval:
    value: 15s
  .ui.enable_login_form:
    value: true
network-properties:
  network:
    name: subnet1
  other_availability_zones:
  - name: us-central1-f
  - name: us-central1-c
  - name: us-central1-b
  singleton_availability_zone:
    name: us-central1-f
resource-config:
  pxc:
    instances: automatic
    persistent_disk:
      size_mb: automatic
    instance_type:
      id: automatic
    internet_connected: true
    max_in_flight: 5
  pxc-proxy:
    instances: automatic
    persistent_disk:
      size_mb: automatic
    instance_type:
      id: automatic
    internet_connected: true
    max_in_flight: 5
  tsdb:
    instances: automatic
    persistent_disk:
      size_mb: automatic
    instance_type:
      id: automatic
    internet_connected: true
    max_in_flight: 1
  ui:
    instances: automatic
    persistent_disk:
      size_mb: automatic
    instance_type:
      id: automatic
    internet_connected: true
    max_in_flight: 5
errand-config:
  smoke-test:
    post-deploy-state: true
  update-admin-password:
    post-deploy-state: true
```

   1. Optional: Change the **Chunk Size (Disk) MB** for calculating the available disk chunks super value metric (SVM). Healthwatch dashboards do not rely on this parameter at all, the purpose of the option is for legacy firehose integration to generate the Diego_AvailableFreeChunksDisk metric only. The legacy firehose integration calculates the metric using Prometheus Query Language (PromQL) and puts them back to the loggregator system to have the calculated metric consumable by third party nozzles.
   1. Optional: Change the **Chunk Size (Memory) MB** for calculating the available memory chunks SVM. The purpose of this option is the same as above.


<p class="note warning"><strong>Warning: </strong>Job Names

When configuring custom scrape jobs, it is recommended that you avoid using the following job names:</p>

      * `Healthwatch-view-pas-exporter`
      * `Healthwatch-view-pks-exporter`
      * `tsdb`
      * `grafana`
      * `pks-master-kube-scheduler`
      * `pks-master-kube-controller-manager`

1. Navigate to **Grafana Configuration** and do the following:
    1. Optional: If you configured Grafana to be accessible from outside the BOSH network, you may want to specify the
       `Root URL For the Grafana`. This should be set to the full URL used to access Grafana from a web browser.
       This property must be set for Generic OAuth or UAA to redirect correctly. It is also used to generate links to Grafana in alert messages.
       The default port Grafana is listening on is 3000, you need to include the port as part of the URL.
       <p class="note"><strong>Note:</strong> The URL you enter here needs to be configured in your DNS provider to point to the public IP address of the Grafana VM, or the public IP address of the load balancer that sits in from of the Grafana VMs. The IP address to Grafana can be obtained after Apply Changes.</p>

    1. Optional: Enter a list of **Static IPs for the Grafana VM(s)**.  These IPs should be comma separated and be 
       valid static IPs from the IAAS for the network on which the tile is installed.
    1. Optional: **Enable Grafana Login Form**  Disabling this option will prevent anyone, including the admin user, from logging in with Basic Authentication
    1. Optional: **Select an authentication mechanism for the Grafana**.  Options include Basic, Generic OAuth(UAA, Github, etc) or LDAP. See [Grafana Authentication](grafana-authentication.html) for detailed configurations.
    1. Optional: **Enable SMTP for Grafana Alerts** Enabling this option will allow Grafana to send emails for alerts that are configured through the dashboard.
       For individual field references, see [Grafana SMTP Configuration](https://grafana.com/docs/installation/configuration/#smtp).
1. Navigate to **Canary URL Configuration** and do the following:
    <p class="note"><strong>Note:</strong>
    Canary applications are those applications you "send in first"
    and are an indicator of the overall health of your platform.
    If a canary app is down,
    there is probably an issue with the platform.
    For example, Apps Manager would be a good candidate for a canary url.
    </p>

    1. Optional: Enter **Exporter Port** for the process to listen on. It is only necessary to change from the default port if
       you have a port conflict on the TSDB VM.  The port is defaulted to `9115`.
    1. Optional: Enter a list of **Target URLs**. [The Blackbox Exporter](https://github.com/prometheus/blackbox_exporter) will run continuous probe tests
       on the URLs and record the results in the TSDB.  These probe tests are useful for generating SLI type metrics. There are no additional scrape configuration jobs necessary
       for the URLs entered here. They will be automatically added to the TSDB scrape jobs.

## PKS Cluster Discovery Configuration

PKS Cluster Discovery enables users to detect and configure clusters created by the PKS API.

To configure PKS cluster discovery: 

<p class="note"><strong>Note:</strong> PKS Cluster Discovery only applies for foundations running PKS</p>

1. Navigate to **PKS Cluster Discovery Configuration** and do the following: 
    1. Select **Enable**
    1. Optional: Enter **Scrape Port** for the process to listen on.
    1. Enter the PKS API Address. This should be the API address you entered in **Ops Manager >Enterprise PKS >PKS API >API Hostname (FQDN)**. For example, `api.pks.example.com`.
    1. Enter the PKS UAA Client and Secret. You can use the PKS management admin client credential, where the client is `admin` and the secret can be found in **Ops Manager >Enterprise PKS>Credentials>Pks Uaa Management Admin Client**.
       Otherwise a separete UAA client with full access to the PKS API can be created and entered here.
    1. Optional: Enter **Test Frequency** to configure the interval in seconds how frequent the PKS SLI test runs. The PKS SLI test monitors the health of PKS API by logging into the API server, listing all clusters, and logging out the API server.
    1. Enter the PKS API Certificate Authority. This is required if the PKS API is using a self-signed certificate. Alternatively, `PKS API Skip SSL Validation` can be selected, but it is not recommended for production use.
    1. At this point you have finished the configuration needed in Healthwatch tile, there are one additional configuration needed in `Enterprise PKS` tile to have the metrics pulling from the clusters correctly.
1. Go to `Enterprise PKS` tile and for each plan you want to monitor:
    1. Navigate to the plan you want to monitor. For example, `Plan 2`.
    1. Scroll to `(Optional) Add-ons - Use with caution`.
    1. Paste the following yaml snippet to the text box to configure roles required to scrape metrics from the clusters:

```yaml
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
 name: healthwatch
rules:
 - resources:
     - pods/proxy
     - pods
     - nodes
     - nodes/proxy
     - namespace/pods
     - endpoints
     - services
   verbs:
     - get
     - watch
     - list
   apiGroups:
     - ""
 - nonResourceURLs: ["/metrics"]
   verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
 name: healthwatch
roleRef:
 apiGroup: ""
 kind: ClusterRole
 name: healthwatch
subjects:
 - apiGroup: ""
   kind: User
   name: healthwatch
   namespace: pks-system
```

If there are already other API resource definitions in the text filed, simply append the above snippet to the end, followed by a newline character.
<p class="note"><strong>Note:</strong> During Apply Changes, Make sure Enterprise PKS > Errands > Upgrade all clusters errand is enabled to have the addon resources propagated to on demand clusters.</p>

1. Return to the Ops Manager Installation Dashboard and click Apply Changes.

1. In order to access Grafana within Healthwatch an external IP address for the `Grafana` VM is needed along with allowing traffic to the VM on port 3000. A native IaaS load balancer can also be used. 

##  (Optional) Backing Up Healthwatch

Healthwatch supports [Bosh Backup and Restore](https://docs.pivotal.io/pivotalcf/2-4/customizing/backup-restore/index.html) functionality.
We recommend running nightly backups via a Concourse pipeline job like the one below:

```yaml
---
resource_types:
  - name: gcs-resource
    type: docker-image
    source:
      repository: frodenas/gcs-resource

  - name: pivnet
    type: docker-image
    source:
      repository: pivotalcf/pivnet-resource
      tag: latest-final

resources:
  - name: bbr-release
    type: pivnet
    source:
      api_token: ((pivnet-refresh-token))
      product_slug: p-bosh-backup-and-restore

  - name: bbr-pipeline-tasks-repo
    type: git
    source:
      uri: https://github.com/pivotal-cf/bbr-pcf-pipeline-tasks.git
      branch: master
      tag_filter: v1.0.0

  - name: bbr-docker
    type: docker-image
    source:
      repository: cloudfoundrylondon/bbr-pipeline
      tag: final

  - name: nightly
    type: time
    source:
      interval: 24h
      start: 22:00
      stop: 00:00
      location: America/Denver

  - name: rv-backup-bucket
    type: gcs-resource
    source:
      bucket: ((gcs-bucket-name))
      json_key: ((service-account-json))
      regexp: ((gcs-directory))/rv-backup-(.*).tar

jobs:
  - name: trigger-backup
    plan:
      - put: nightly

  - name: nightly-backup
    plan:
      - aggregate:
        - get: nightly
          trigger: true
        - get: bbr-docker
        - get: bbr-release
        - get: bbr-pipeline-tasks-repo
      - task: extract-binary
        file: bbr-pipeline-tasks-repo/tasks/extract-bbr-binary/task.yml
      - task: run-backup
        image: bbr-docker
        config:
          platform: linux

          inputs:
            - name: binary
            - name: bbr-pipeline-tasks-repo

          outputs:
            - name: rv-backup-artifact

          params:
            OPSMAN_URL: ((opsman-url))
            OPSMAN_USERNAME: ((opsman-user.username))
            OPSMAN_PASSWORD: ((opsman-user.password))
            OPSMAN_PRIVATE_KEY: ((ops-man-ssh-key.private_key))

          run:
            path: /bin/bash
            args:
              - -c
              - |
                #!/usr/bin/env bash

                set -eu

                scripts="${PWD}/bbr-pipeline-tasks-repo/scripts"

                export CLIENT_ID=
                # shellcheck disable=SC1090
                source "${scripts}/export-director-metadata"

                om_cmd curl -p /api/v0/deployed/products > deployed_products.json
                DEPLOYMENT_NAME=$(jq -r '.[] | select(.type == "p-Healthwatch-view") | .guid' "deployed_products.json")
                export DEPLOYMENT_NAME

                pushd rv-backup-artifact
                  ${scripts}/deployment-backup
                  tar -cvf rv-backup-$(date +%Y.%m.%d.%H.%M.%S).tar -- *
                popd

      - put: rv-backup-bucket
        params:
          file: rv-backup-artifact/rv-backup-*.tar
```

##  (Optional) Restoring Healthwatch from a BBR Backup
If you are running nightly backups, you have the ability to restore the TSDB and Grafana data using Bosh Backup and Restore.
Be aware that this will delete any existing data on all TSDB and Grafana VMs and restore them to when the backup was taken.

To restore, run the following commands:

```bash
mkdir -p $PATH_TO_DEPLOYMENT_BACKUP
tar xvf rv-backup-*.tar -C $PATH_TO_DEPLOYMENT_BACKUP
bbr deployment \
  --target $BOSH_TARGET \
  --username $BOSH_CLIENT \
  --password $BOSH_PASSWORD \
  --deployment $Healthwatch_VIEW_DEPLOYMENT_NAME \
  --ca-cert $PATH_TO_BOSH_SERVER_CERTIFICATE \
  restore \
  --artifact-path $PATH_TO_DEPLOYMENT_BACKUP
```
