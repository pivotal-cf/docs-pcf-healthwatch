---
title: Healthwatch v2.3 Release Notes
owner: Healthwatch
---

This topic contains release notes for Healthwatch&trade; for VMware Tanzu^&reg;^ v2.2 (Healthwatch).

For information about the risks and limitations of Healthwatch v2.3, see [Assumed Risks of Using Healthwatch v2.3](index.html#assumed-risks) and [Healthwatch
v2.3 Limitations](index.html#healthwatch-limitations) in _Healthwatch for VMware Tanzu_.

## <a id='releases'></a> Releases

### <a id='2-3-0'></a> v2.3.0

**Release Date:** April 15, 2024

* **[Breaking Change]** Healthwatch now requires the use of [system-metrics-agent](https://github.com/cloudfoundry/system-metrics-release/blob/main/docs/system-metrics-agent.md)
processes to gather "system" metrics from BOSH deployed VMs.  Please make sure to enable `Enable System Metrics`
in the `Director Config` tab of the BOSH Director tile. Ensure that `Apply Changes` is run on all tiles and that
all VMs deployed via service brokers are upgraded prior to upgrading Healthwatch. The impact of not performing this
action is that Healthwatch dashboards will fail to populate with metrics about VM health, cpu, memory, disk and other
statistics. Alerts may also be affected. Any custom dashboards that query "system" metrics with an `origin` label value
of `bosh-system-metrics-forwarder` will need to be updated to use `system-metrics-agent` as the `origin` label value.

* **[Feature]** Added support of VMware Tanzu Kubernetes Grid Integrated Edition v1.16.

Healthwatch v2.3.0 uses the following open-source component versions:

| Component    | Packaged Version |
|--------------|------------------|
| Prometheus   | 2.51.0           |
| Grafana      | 10.4.1           |
| Alertmanager | 0.27.0           |
| PXC          | 1.0.25           |


## <a id='upgrade'></a> How to Upgrade

To upgrade from Healthwatch v2.1 to Healthwatch v2.2, see [Upgrading Healthwatch](upgrading-healthwatch.html).


## <a id='new-features'></a> New Features

Healthwatch v2.2 includes the following major features:

### <a id='routing-rules-auto-config'></a> Default Routing Rules Are Pre-Configured for Alertmanager

In new installations of Healthwatch, the **Routing rules** field in the **Alertmanager** pane of the Healthwatch tile is pre-configured with a default set of
routing rules. You can edit these routing rules according to the needs of your deployment.

For more information about configuring routing rules for Alertmanager, see [Configure Alerting](configuring/optional-config/alerting.html#configure) in
_Configuring Alerting_.

### <a id='grafana-8'></a> Healthwatch Requires New Open Source License for Grafana v8

Healthwatch uses Grafana v8, which requires the Affero General Public License (AGPL).

For more information about the AGPL, see [GNU Affero General Public License](https://www.gnu.org/licenses/agpl-3.0.en.html) on the GNU site. For more
information about Grafana v8, see the [Grafana documentation](https://grafana.com/docs/grafana/latest/whatsnew/whats-new-in-v8-0/).

### <a id='grafana-route-auto-config'></a> Automatic Grafana UI Route Configuration

If your Ops Manager foundation has TAS for VMs installed, you can configure Healthwatch to automatically create a route for the Grafana UI in the **Grafana**
pane of the Healthwatch tile.

For more information about configuring a route for the Grafana UI, see [(Optional) Configure Grafana](configuring/configuring-healthwatch.html#grafana) in
_Configuring Healthwatch_.

### <a id='uaa-auth-auto-config'></a> Automatic UAA Authentication Configuration

When you select **UAA** as your Grafana UI authentication method in the **Grafana Authentication** pane of the Healthwatch tile, Healthwatch automatically
configures authentication with the UAA instances in TAS for VMs and TKGI for the Grafana UI. If you want to configure authentication with a UAA instance on a
different Ops Manager foundation, you must select **Generic OAuth** and configure it manually through the **Grafana Authentication** pane.

For more information about configuring UAA as your Grafana UI authentication method, see [(Optional) Configure Grafana
Authentication](configuring/configuring-healthwatch.html#authentication) in _Configuring Healthwatch_.

### <a id='grafana-logout-url'></a> Grafana UI Logout URL

If you configured a generic OAuth provider to authenticate users who log in to the Grafana UI, you can configure a logout URL in the **Grafana
Authentication** pane of the Healthwatch tile.

For more information about configuring a logout URL for the Grafana UI, see [Configure Generic OAuth
Authentication](configuring/grafana-authentication.html#configure-generic-oauth) in _Configuring Grafana Authentication_.

### <a id='remove-grafana'></a> Remove Grafana

If you do not want to use any Grafana instances in your Healthwatch deployment, you can set the number of Grafana, MySQL, and MySQL Proxy instances for your
Healthwatch deployment to `0` in the **Resource Config** pane of the Healthwatch tile.

For more information about removing Grafana from your Healthwatch deployment, see [Removing Grafana](resources.html#remove-grafana) in _Healthwatch Components
and Resource Requirements_.

### <a id='bearer-token-remote'></a> Prometheus Can Log In to Remote Storage with a Bearer Token

In the **Remote Write** pane of the Healthwatch tile, you can configure the Prometheus instance to use a bearer token to log in to a remote storage endpoint.

For more information about configuring the Prometheus instance to use a bearer token to log in to a remote storage endpoint, see [(Optional) Configure Remote
Write](configuring/configuring-healthwatch.html#remote-write) in _Configuring Healthwatch_.

### <a id='cluster-discovery-auto-config'></a> Automatic TKGI Cluster Discovery Configuration

Healthwatch automatically configures TKGI cluster discovery by default on Ops Manager foundations that have TKGI installed. If you do not want Healthwatch to
configure TKGI cluster discovery, you can disallow it through the **TKGI Cluster Discovery** pane in the Healthwatch tile.

For more information about TKGI cluster discovery, see [Configuring TKGI Cluster Discovery](configuring/configuring-cluster-discovery.html). For more
information about allowing or disallowing TKGI cluster discovery, see [Configure TKGI Cluster Discovery in
Healthwatch](configuring/configuring-cluster-discovery.html#configure-tkgi-cluster-discovery) in _Configuring TKGI Cluster Discovery_.

### <a id='system-at-a-glance'></a> System at a Glance Dashboard in the Grafana UI

The Grafana UI includes the **System at a Glance** dashboard. This dashboard displays an overview of metrics related to the health of your Ops Manager
foundation and the runtimes you have installed on that foundation.

For more information about the **System at a Glance** dashboard, see [Default Dashboards in the Grafana UI](using-grafana.html#dashboards) in _Using
Healthwatch Dashboards in the Grafana UI_.

### <a id='bosh-director-direct-scrape'></a> Prometheus Scrapes Metrics Directly from the BOSH Director VM

For Ops Manager v2.10.10 and later, the Prometheus instance scrapes BOSH Director metrics directly from the BOSH Director VM instead of the Loggregator
Firehose. This allows the Prometheus VM to gather more types of metrics related to the health of the BOSH Director. These metrics appear in the **Director
Health** dashboard in the Grafana UI.

For more information about the BOSH Director metrics that the Prometheus instance scrapes, see [BOSH SLIs](metrics.html#bosh-sli) in _Healthwatch Metrics_.

### <a id='bosh-deployment-metric'></a> BOSH Deployments Status Emitted Into the Loggregator Firehose

When the SVM Forwarder VM and the BOSH deployment metric exporter VM are both deployed in the Healthwatch Exporter for TAS for VMs tile, the SVM Forwarder VM
emits the `bosh_deployments_status` metric into the Loggregator Firehose.

For more information about the `bosh_deployments_status` metric, see [BOSH Deployment Metric Exporter VM](metrics.html#bosh-deployments-exporter) in
_Healthwatch Metrics_. For more information about the BOSH deployment metric exporter VM, see [(Optional) Configure the BOSH Deployment Metric Exporter
VM](configuring/configuring-exporter-tas.html#bosh-deployments) in _Configuring Healthwatch Exporter for TAS for VMs_. For more information about the SVM
Forwarder VM, see [(Optional) Configure Resources](configuring/configuring-exporter-tas.html#resource-config) in _Configuring Healthwatch Exporter for TAS
for VMs_.

### <a id='om-canary-test-auto-config'></a> Healthwatch Automatically Runs Canary Tests for the Ops Manager Installation Dashboard

Healthwatch automatically runs canary tests for the Ops Manager Installation Dashboard.

For more information about canary test metrics, see [Prometheus VM](metrics.html#tsdb) in _Healthwatch Metrics_.

### <a id='current-canary-apps'></a> Grafana UI Dashboards Only Include Metrics for Current Canary Apps

Dashboards in the Grafana UI only show metrics for canary apps that are currently configured. Metrics for canary apps that are no longer used in your
Healthwatch deployment are removed from your dashboards, in order to avoid mixing outdated data with current data.

For more information about canary test metrics, see [Prometheus VM](metrics.html#tsdb) in _Healthwatch Metrics_.

### <a id='tas-sli-test-timeout'></a> TAS for VMs SLI Test Timeouts Are Increased

The timeouts for the TAS for VMs SLI test suite are increased to five minutes. This reduces the number of false positives you may see in your metrics data.

For more information about canary test metrics, see [TAS for VMs SLI Exporter VM](metrics.html#pas-sli-exporter) in _Healthwatch Metrics_.

### <a id='probe-metrics'></a> Two Canary Test Metrics Emitted Into the Loggregator Firehose

If you deploy the SVM Forwarder VM in the Healthwatch Exporter for TAS for VMs tile, the SVM Forwarder VM emits the `probe_success` and
`probe_duration_seconds` canary test metrics into the Loggregator Firehose.

For more information about canary test metrics, see [Prometheus VM](metrics.html#tsdb) in _Healthwatch Metrics_. For more information about the SVM Forwarder
VM, see [(Optional) Configure Resources](configuring/configuring-exporter-tas.html#resource-config) in _Configuring Healthwatch Exporter for TAS for VMs_.


## <a id='breaking-changes'></a> Breaking Changes

Healthwatch v2.2 includes the following breaking changes:

### <a id='jammy-stemcell'></a> Healthwatch Uses Ubuntu Jammy Stemcell 1.49 or Later

To use Healthwatch v2.2.5 or later, you must upgrade its stemcell to Ubuntu Jammy Stemcell 1.49 or later. You can download Ubuntu Jammy Stemcell 1.49 and
later from [VMware Tanzu Network](https://network.pivotal.io/products/stemcells-ubuntu-jammy#/releases/1206026).

For more information about supported stemcells, see the [Stemcells for VMware Tanzu
documentation](https://docs.vmware.com/en/Stemcells-for-VMware-Tanzu/services/release-notes/index.html).

### <a id='scripts'></a> Update Automation Scripts

Many configuration options have been added, changed, or removed for Healthwatch v2.2. If you use automated scripts to install and configure Healthwatch, you
must update your scripts to reflect the new configuration requirements.

For more information about installing and configuring Healthwatch through platform automation, see [Installing, Configuring, and Deploying a Tile Through an
Automated Pipeline](installing/automated-pipeline.html).

### <a id='breaking-uaa'></a> Authenticating with a UAA Instance on a Different Ops Manager Foundation

If you are upgrading from Healthwatch v2.1 and configured **UAA** as your authentication method for logging in to the Grafana UI, Healthwatch v2.2 keeps
**UAA** as your configured authentication method by default. If you configured a UAA instance on a different Ops Manager foundation as the authentication
method for logging in to the Grafana UI in Healthwatch v2.1, you must select **Generic OAuth** and configure the settings for the external UAA instance in the
**Grafana Authentication** pane.

For more information about configuring a UAA instance on a different Ops Manager foundation as the authentication method for logging in to the Grafana UI, see
[Configuring Authentication with a UAA Instance on a Different Ops Manager Foundation](configuring/optional-config/configuring-external-uaa.html).

### <a id='pas-exporter-timer'></a> Timer Metric Exporter VM is Removed

The timer metric exporter VM, `pas-exporter-timer`, is removed from Healthwatch Exporter for TAS for VMs. This removes unnecessary data and uses fewer IaaS
resources.

For more information about the metrics for TAS for VMs that Healthwatch Exporter for TAS for VMs collects, see [Healthwatch Exporter for TAS for VMs Metric
Exporter VMs](metrics.html#pas-exporters) in _Healthwatch Metrics_.

### <a id='tkgi-1-13'></a> Healthwatch v2.2.1 Requires Additional Configuration in TKGI v1.13

After you install Healthwatch v2.2.1, you must configure TKGI v1.13 to send metrics for Kubernetes Controller Manager to Healthwatch.

For more information about configuring TKGI v1.13 to send metrics for Kubernetes Controller Manager to Healthwatch, see [Configure
TKGI](configuring/configuring-cluster-discovery.html#configure-tkgi) in _Configuring TKGI Cluster Discovery_.


## <a id='known-issues'></a> Known Issues

Healthwatch v2.2 includes the following known issues:

### <a id='bbr-sdk-false-scan'></a> BBR-SDK Can Trigger False Positives in Malware Scans

In Healthwatch v2.2.5, compiling BBR-SDK on Ubuntu Jammy stemcells can cause it to trigger a false positive in some McAfee malware scans. This false positive
incorrectly identifies the
`releases/backup-and-restore-sdk-1.18.56-ubuntu-jammy-1.18-20221101-135320-248952853/compiled_packages/ database-backup-restorer-postgres-13/lib/earthdistance.so`
file as infected.

However, when you compile BBR-SDK on Ubuntu Xenial stemcells, it does not trigger any alerts in McAfee malware scans.

For more information about BBR-SDK, see the [BOSH
documentation](https://bosh.io/releases/github.com/cloudfoundry-incubator/backup-and-restore-sdk-release?all=1).

### <a id='metric-name-recursion'></a> SVM Forwarder Creates Recursive Metric Labels

This known issue is fixed in Healtwatch v2.2.1 and later.

When the SVM Forwarder VM is deployed in Healthwatch Exporter for TAS for VMs, a change in the Prometheus server causes metrics with the `job` and
`exported_job` labels to become recursive. For example, `exported_job` becomes `exported_exported_exported_exported_job`.

To work around this issue, set the number of SVM Forwarder VM instances for your Healthwatch deployment to `0` in the **Resource Config** pane of the
Healthwatch Exporter for TAS for VMs tile. For more information about scaling Healthwatch resources, see [(Optional) Configure
Resources](configuring/configuring-exporter-tas.html#resource-config) in _Configuring Healthwatch Exporter for TAS for VMs_.

### <a id='no-data-nodes-tkgi-112'></a> No Data for containerd Clusters on Kubernetes Nodes Dashboard for TKGI v1.12 and Later

This known issue is fixed in Healthwatch v2.2.2 and later.

If you have TKGI v1.12 or later installed, the **Kubernetes Nodes** dashboard in the Grafana UI might not show data for Kubernetes clusters that use the
containerd runtime.

In TKGI v1.11 and earlier, the `name` label in Kubernetes cluster metrics start with `k8s_`. However, in TKGI v1.12 and later, new Kubernetes clusters run on
containerd instead of in Docker. As a result, in TKGI v1.12 and above the `name` label in Kubernetes cluster metrics start with a hex value instead of `k8s_`,
which the Grafana instance does not recognize.

To fix this issue, upgrade to Healthwatch v2.2.2 or later.

### <a id='no-data-nodes-tkgi-110'></a> No Data for Individual Pods on Kubernetes Nodes Dashboard for TKGI v1.10

If you are using TKGI v1.10.0 or v1.10.1, the **Kubernetes Nodes** dashboard in the Grafana UI might not show data for individual pods. This is due to a known
issue in Kubernetes v1.19.6 and earlier and Kubernetes v1.20.1 and earlier.

To fix this issue, upgrade to TKGI v1.10.2 or later. For more information about upgrading to TKGI v1.10.2 or later, see the [TKGI
documentation](https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid/index.html).

### <a id='tkgi-windows-clusters'></a> No Data on Kubernetes Nodes Dashboard for Windows Clusters

If you are using TKGI to monitor Windows clusters, the **Kubernetes Nodes** dashboard in the Grafana UI might not show data. Healthwatch does not currently
visualize node metrics for Windows clusters.

### <a id='tkgi-service-accounts'></a> Healthwatch Exporter for TKGI Does Not Clean Up TKGI Service Accounts

This known issue is fixed in Healthwatch v2.2.1 and later.

If you run SLI tests for TKGI through Healthwatch Exporter for TKGI, and you do not have an OpenID Connect (OIDC) provider for your Kubernetes clusters
configured for TKGI, the TKGI SLI exporter VM does not automatically clean up the service accounts that it creates while running the TKGI SLI test suite.

To fix this issue, either upgrade to Healthwatch v2.2.1 or configure an OIDC provider as the identity provider for your Kubernetes clusters in the TKGI tile.
This cleans up the service accounts that the TKGI SLI exporter VM creates in future TKGI SLI tests, but does not clean up existing service accounts from 
previous TKGI SLI tests. For more information about configuring an OIDC provider in TKGI, 
see [OIDC Provider for Kubernetes Clusters](https://docs.vmware.com/en/VMware-Tanzu-Kubernetes-Grid-Integrated-Edition/1.16/tkgi/GUID-oidc-provider.html) 
in the TKGI documentation.  

You may need to manually delete existing service accounts from previous TKGI SLI tests. For more information about manually deleting existing service
accounts, see [Healthwatch Exporter for TKGI Does Not Clean Up TKGI Service Accounts](troubleshooting.html#tkgi-service-accounts) in _Troubleshooting
Heathwatch_.

### <a id='bbr-backups-tsdb'></a> BBR Backup Snapshots Fill Disk Space on Prometheus VMs

This known issue is fixed in Healthwatch v2.2.1 and later.

In Healthwatch v2.2.0, the backup scripts for Prometheus VMs do not clean up the intermediary snapshots created by BBR. This results in the disk space on
Prometheus VMs filling up.

To fix this issue, either upgrade to Healthwatch v2.2.1 or manually clean up the snapshots. For more information about manually cleaning up the snapshots, see
[BBR Backup Snapshots Fill Disk Space on Prometheus VMs](troubleshooting.html#bbr-backups-tsdb) in _Troubleshooting Healthwatch_.

### <a id='rabbitmq-tls'></a> No Data on RabbitMQ Dashboards for RabbitMQ On-Demand Instances Using TLS

This known issue is fixed in Healthwatch v2.2.2 and later.

In Healthwatch v2.2.1 and earlier, the Prometheus instance does not scrape metrics from RabbitMQ on-demand instances that are configured to communicate over
TLS. As a result, the **RabbitMQ** dashboards in the Grafana UI show no data for RabbitMQ on-demand instances that are configured to use TLS.

To fix this issue, upgrade to Healthwatch v2.2.2 or later and RabbitMQ v2.0.13 or later.

### <a id='no-data-tsdb-update'></a> No Data While Re-Deploying Highly-Available Healthwatch Installations

This known issue is fixed in Healthwatch v2.2.3 and later.

In Healthwatch v2.2.2 and earlier, the Grafana instance cannot load metrics data in the Grafana UI after you re-deploy an HA Healthwatch installation with
multiple Prometheus instances. An HA Healthwatch installation is meant to allow the Grafana instance to continue loading data during re-deployment by ensuring
that the second Prometheus instance does not start updating until after the first Prometheus instance has updated and re-starts. In Healthwatch v2.2.2 and
earlier, a bug causes the second Prometheus instance to start updating before the first Prometheus instance re-starts.

To fix this issue, upgrade to Healthwatch v2.2.3 or later.

### <a id='prom-smoke-test'></a> Prometheus Smoke Test Fails as Healthwatch Re-Deploys

This known issue is fixed in Healthwatch v2.2.2 and later.

In Healthwatch v2.2.1 and earlier, a potential race condition sometimes causes the smoke test for Prometheus VMs to run before the Prometheus VM is ready.
This leads to the smoke test failing when you re-deploy Healthwatch, even though it succeeds when you run the smoke test manually.

### <a id='prom-wal-error'></a> Prometheus Clean-Up Failure Leads to Full Disk

This known issue is fixed in Healthwatch v2.2.2 and later.

In Healthwatch v2.2.1, under rare circumstances, the Prometheus instance fails to clean up the `chunks_head` directory. This leads to a full disk and
subsequent failures when the Prometheus instance attempts to process new metrics.

To fix this issue, upgrade to Healthwatch v2.2.2 or later.

### <a id='user-already-exists-error'></a> User already exists error during UAA authentication after upgrading Healthwatch

This known issue is fixed in Healthwatch v2.2.8 and later.

After upgrading Healthwatch, a `user already exists` error occurs while attempting to log in to Grafana using
UAA authentication. A new option has been added to the Grafana configuration file.  

To fix this issue, upgrade Healthwatch to v2.2.8 or later.
