---
title: Configuring Healthwatch Exporter for Tanzu Platform for CF
owner: Healthwatch
---

This topic describes how to manually configure and deploy the Healthwatch Exporter for VMware <%= vars.runtime_prod_full %> tile.

To install, configure, and deploy Healthwatch Exporter for <%= vars.runtime_prod_full %> through an automated pipeline, see [Installing, Configuring, and Deploying a Tile
Through an Automated Pipeline](../installing/automated-pipeline.html).


## <a id='overview'></a> Overview of Configuring and Deploying Healthwatch Exporter for <%= vars.runtime_prod_full %>

When installed on an Tanzu Operations Manager foundation you want to monitor, Healthwatch Exporter for <%= vars.runtime_prod_full %> deploys metric exporter VMs to generate each type of
metric related to the health of your <%= vars.runtime_prod_short %> deployment. Healthwatch Exporter for <%= vars.runtime_prod_full %> sends metrics through the Loggregator Firehose to a
Prometheus exposition endpoint on the associated metric exporter VMs. The Prometheus instance that exists within your metrics monitoring system then scrapes
the exposition endpoints on the metric exporter VMs and imports those metrics into your monitoring system. For more information about the architecture of the
Healthwatch Exporter for TAS for VMs tile, see [Healthwatch Exporter for <%= vars.runtime_prod_full %>](../architecture.html#healthwatch-tas) in _Healthwatch Architecture_.

After installing Healthwatch Exporter for <%= vars.runtime_prod_full %>, you configure the metric exporter VMs deployed by Healthwatch Exporter for <%= vars.runtime_prod_full %> through the tile
UI. You can also configure errands and system logging, as well as scale VM instances up or down and configure load balancers for multiple VM instances.

To configure and deploy the Healthwatch Exporter for TAS for VMs tile:

<p class="note">
If you want to quickly deploy the Healthwatch Exporter for TAS for VMs tile to ensure that it deploys successfully
before you fully configure it, you only need to configure the <strong>Assign AZ and Networks</strong> and <strong>BOSH Health Metric Exporter VM</strong>
panes.</p>

1. Navigate to the Healthwatch Exporter for TAS for VMs tile in the Ops Manager Installation Dashboard. For more information, see [Navigate to the Healthwatch
Exporter for TAS for VMs Tile](#open-ui) below.

1. Assign jobs to your availability zones (AZs) and networks. For more information, see [Assign AZs and Networks](#az) below.

2. (Optional) Configure the **TAS for VMs Metric Exporter VMs** pane. For more information, see [(Optional) Configure <%= vars.runtime_prod_full %> Metric Exporter VMs](#exporter-config) below.

1. Configure the **BOSH Health Metric Exporter VM** pane. For more information, see [Configure the BOSH Health Metric Exporter VM](#bosh-exporter) below.

2. (Optional) Configure the **BOSH Deployment Metric Exporter VM** pane. For more information, see [(Optional) Configure the BOSH Deployment Metric Exporter
VM](#bosh-deployments) below.

1. (Optional) Configure the **Errands** pane. For more information, see [(Optional) Configure Errands](#errands) below.

2. (Optional) Configure the **Syslog** pane. For more information, see [(Optional) Configure Syslog](#syslog) below.

3. (Optional) Configure the **Resource Config** pane. For more information, see [(Optional) Configure Resources](#resource-config) below.

4. Deploy the Healthwatch Exporter for TAS for VMs tile through the Ops Manager Installation Dashboard. For more information, see [Deploy Healthwatch Exporter
for <%= vars.runtime_prod_full %>](#apply-changes) below.

1. Once you have finished installing, configuring, and deploying Healthwatch Exporter for TAS for VMs, configure a scrape job for Healthwatch Exporter for TAS
for VMs in the Prometheus VM that exists within your monitoring system. For more information, see [Configure a Scrape Job for Healthwatch Exporter for TAS for
VMs](#scrape-job-config) below.

    <p class="note">
    You only need to configure a scrape job for installations of Healthwatch Exporter for TAS for VMs that are not on the
    same Tanzu Operations Manager foundation as your Healthwatch for VMware Tanzu tile. The Prometheus instance in the Healthwatch tile automatically discovers and
    scrapes Healthwatch Exporter tiles that are installed on the same Tanzu Operations Manager foundation as the Healthwatch tile.</p>


## <a id='open-ui'></a> Navigate to the Healthwatch Exporter for TAS for VMs Tile

To navigate to the Healthwatch Exporter for TAS for VMs tile:

1. Navigate to the Ops Manager Installation Dashboard.

1. Click the **Healthwatch Exporter for Tanzu Application Service** tile.


## <a id='az'></a> Assign AZs and Networks

In the **Assign AZ and Networks** pane, you assign jobs to your AZs and networks.

To configure the **Assign AZ and Networks** pane:

1. Select **Assign AZs and Networks**.

1. Under **Place singleton jobs in**, select the first AZ. Tanzu Operations Manager runs any job with a single instance in this AZ.

1. Under **Balance other jobs in**, select one or more other AZs. Tanzu Operations Manager balances instances of jobs with more than one instance across the AZs that you
specify.

1. From the **Network** dropdown, select the runtime network that you created when configuring the BOSH Director tile. For more information about <%= vars.runtime_prod_full %>
networks, see the [Tanzu Operations Manager documentation](https://techdocs.broadcom.com/us/en/vmware-tanzu/platform/tanzu-operations-manager/<%= vars.opsman_version %>/tanzu-ops-manager/refarch-index.html#pas-subnets).

1. (Optional) If you want to assign jobs to a service network in addition to your runtime network, select it from the **Services Network** dropdown. For more
information about <%= vars.runtime_prod_full %> service networks, see the [Tanzu Operations Manager documentation](https://techdocs.broadcom.com/us/en/vmware-tanzu/platform/tanzu-operations-manager/<%= vars.opsman_version %>/tanzu-ops-manager/refarch-index.html#pas-subnets).

1. Click **Save**.


## <a id='exporter-config'></a> (Optional) Configure <%= vars.runtime_prod_full %> Metric Exporter VMs

In the **TAS for VMs Metric Exporter VMs** pane, you configure static IP addresses for the metric exporter VMs that collect metrics from the Loggregator
Firehose in <%= vars.runtime_prod_full %>. There are two metric exporter VMs that each collect a single metric type from the Loggregator Firehose: counter or gauge. You can
deploy one or both VMs. After generating these metrics, the metric exporter VMs convert them to a Prometheus exposition format on a secured endpoint.

You can also deploy two other VMs: the <%= vars.runtime_prod_full %> service level indicator (SLI) exporter VM and the certificate expiration metric exporter VM.

To configure the **TAS for VMs Metric Exporter VMs** pane:

<p class="note important">
The IP addresses you configure in the <strong>TAS for VMs Metric Exporter VMs</strong> pane must not be
within the reserved IP ranges you configured in the BOSH Director tile.</p>

1. Select **TAS for VMs Metric Exporter VMs**.

1. (Optional) For **Static IP address for counter metric exporter VM**, enter a valid static IP address that you want to reserve for the counter metric
exporter VM.

1. (Optional) For **Static IP address for gauge metric exporter VM**, enter a valid static IP address that you want to reserve for the gauge metric exporter
VM.

1. (Optional) For **Static IP address for TAS for VMs SLI exporter VM**, enter a valid static IP address that you want to reserve for the <%= vars.runtime_prod_full %> SLI
exporter VM. The <%= vars.runtime_prod_full %> SLI exporter VM generates SLIs that allow you to monitor whether the core functions of the Cloud Foundry Command-Line Interface
(cf CLI) are working as expected. The cf CLI allows developers to create and manage apps through <%= vars.runtime_prod_full %>. For more information, see [<%= vars.runtime_prod_full %> SLI
Exporter VM](../metrics.html#pas-sli-exporter) in _Healthwatch Metrics_.

1. (Optional) For **Static IP address for certificate expiration metric exporter VM**, enter a valid static IP address that you want to reserve for the
certificate expiration metric exporter VM. The certificate expiration metric exporter VM collects metrics that show when certificates in your Tanzu Operations Manager
deployment are due to expire. For more information, see [Certificate Expiration Metric Exporter VM](../metrics.html#cert-expiration-exporter) in _Healthwatch
Metrics_ and [Monitoring Certificate Expiration](optional-config/certificate-monitoring.html).

    <p class="note">
    If you have both Healthwatch Exporter for <%= vars.runtime_prod_full %> and Healthwatch Exporter for TKGI installed on the same Tanzu Operations Manager foundation, scale the certificate expiration metric exporter VM to zero instances in the <strong>Resource Config</strong> pane in one of the Healthwatch Exporter tiles. Otherwise, the two certificate expiration metric exporter VMs create redundant sets of metrics.</p>

1. (Optional) If your Tanzu Operations Manager deployment uses self-signed certificates, activate the **Skip TLS certificate verification for certificate metric exporter
VM** checkbox. When this checkbox is activated, the certificate expiration metric exporter VM does not verify the identity of the Tanzu Operations Manager VM. This
checkbox is deactivated by default.

1. Under **cf CLI version**, select from the dropdown the version of the cf CLI that your <%= vars.runtime_prod_full %> deployment uses:
    * If you have TAS for VMs v2.11 or later installed, select one of the following options:
      * **7 (Use with TAS 2.11+):** Allows the <%= vars.runtime_prod_full %> SLI exporter VM to run SLI tests for cf CLI v7. This option is selected by default.
      * **CF CLI 8:** Allows the <%= vars.runtime_prod_full %> SLI exporter VM to run SLI tests for cf CLI v8.

    <p class="note">
    Only Tanzu Application Service versions 4.0 and later are supported This includes <%= vars.runtime_prod_full %> 10.0+.</p>

2. (Optional) If `Metric Registrar` is configured in your <%= vars.runtime_prod_full %> tile, and you do **not** want Healthwatch to scrape custom application metrics,
select the **Filter out custom application metrics** checkbox.

    <p class="note">
    If you configured Healthwatch to receive data through the OpenTelemetry Collector for Tanzu Application Service 6.0 and above and made changes to the <strong>Filter out custom application metrics</strong> check box, you need to apply the changes to the Healthwatch tile. For detailed instructions, see
    <a href="configuring-healthwatch.html.md.erb#deploy-healthwatch">Deploy Healthwatch</a>
    </p>

1. Click **Save**.


## <a id='bosh-exporter'></a> Configure the BOSH Health Metric Exporter VM

In the **BOSH Health Metric Exporter VM** pane, you configure the AZ and VM type of the BOSH health metric exporter VM. Healthwatch Exporter for TAS for VMs
deploys the BOSH health metric exporter VM, which creates a BOSH deployment called `bosh-health` every ten minutes. The `bosh-health` deployment deploys
another VM, `bosh-health-check`, that runs a suite of SLI tests to validate the functionality of the BOSH Director. After the SLI tests are complete, the BOSH
health metric exporter VM collects the metrics from the `bosh-health-check` VM, then deletes the `bosh-health` deployment and the `bosh-health-check` VM. For
more information, see [BOSH Health Metric Exporter VM](../metrics.html#bosh-health-exporter) in _Healthwatch Metrics_.

To configure the **BOSH Health Metric Exporter VM** pane:

1. Select **BOSH Health Metric Exporter VM**.

1. Under **Availability zone**, select the AZ on which you want Healthwatch Exporter for TAS for VMs to deploy the BOSH health metric exporter VM.

1. Under **VM type**, select from the dropdown the type of VM you want Healthwatch Exporter for TAS for VMs to deploy.

1. Click **Save**.

    <p class="note">
    If you have both Healthwatch Exporter for TAS for VMs and Healthwatch Exporter for TKGI installed on the same Ops
    Manager foundation, scale the BOSH health metric exporter VM to zero instances in the <strong>Resource Config</strong> pane in one of the Healthwatch
    Exporter tiles. Otherwise, the two sets of BOSH health metric exporter VM metrics cause a <code>401</code> error in your BOSH Director deployment, and one
    set of metrics reports that the BOSH Director is down in the Grafana UI. For more information, see
    <a href="../troubleshooting.html#bosh-health-metric-errors">BOSH Health Metrics Cause Errors When Two Healthwatch Exporter Tiles Are Installed</a> in
    <em>Troubleshooting Healthwatch</em>.</p>


## <a id='bosh-deployments'></a> (Optional) Configure the BOSH Deployment Metric Exporter VM

In the **BOSH Deployment Metric Exporter VM** pane, you configure the authentication credentials and a static IP address for the BOSH deployment metric
exporter VM. This VM checks every 30 seconds whether any BOSH deployments other than the one created by the BOSH health metric exporter VM are running. For
more information, see [BOSH Deployment Metric Exporter VM](../metrics.html#bosh-deployments-exporter) in _Healthwatch Metrics_.

To configure the **BOSH Deployment Metric Exporter VM** pane:

1. Select **BOSH Deployment Metric Exporter VM**.

1. (Optional) For **UAA client credentials**, enter the username and secret for the UAA client that the BOSH deployment metric exporter VM uses to access the
BOSH Director VM. For more information, see [Create a UAA Client for the BOSH Deployment Metric Exporter VM](#create-uaa-client) below.

1. (Optional) For **Static IP address for BOSH deployment metric exporter VM**, enter a valid static IP address that you want to reserve for the BOSH
deployment metric exporter VM. This IP address must not be within the reserved IP ranges you configured in the BOSH Director tile.

1. Click **Save**.

<p class="note">
If you have both Healthwatch Exporter for TAS for VMs and Healthwatch Exporter for TKGI installed on the same Ops
Manager foundation, scale the BOSH deployment metric exporter VM to zero instances in the <strong>Resource Config</strong> pane in one of the Healthwatch
Exporter tiles. Otherwise, the two BOSH deployment metric exporter VMs create redundant sets of metrics.</p>

### <a id='create-uaa-client'></a> Create a UAA Client for the BOSH Deployment Metric Exporter VM

To allow the BOSH deployment metric exporter VM to access the BOSH Director VM and view BOSH deployments, you must create a new UAA client for the BOSH
deployment metric exporter VM. The procedure to create this UAA client differs depending on the authentication settings of your Tanzu Operations Manager deployment.

To create a UAA client for the BOSH deployment metric exporter VM:

1. Return to the Ops Manager Installation Dashboard.

1. Record the IP address for the BOSH Director VM and the login and admininistrator credentials for the BOSH Director UAA instance:
    * If your Tanzu Operations Manager deployment uses internal authentication:
        1. Click the **BOSH Director** tile.
        1. Select the **Status** tab.
        1. Record the IP address in the **IPs** column of the **BOSH Director** row.
        1. Select the **Credentials** tab.
        1. In the **Uaa Admin Client Credentials** row of the **BOSH Director** section, click **Link to Credential**.
        1. Record the value of `password`. This value is the secret for **Uaa Admin Client Credentials**.
        1. Return to the **Credentials** tab.
        1. In the **Uaa Login Client Credentials** row of the **BOSH Director** section, click **Link to Credential**.
        1. Record the value of `password`. This value is the secret for **Uaa Login Client Credentials**.
        <br>
        <br>
      For more information about internal authentication settings for your Tanzu Operations Manager deployment, see the [Tanzu Operations Manager documentation](https://techdocs.broadcom.com/us/en/vmware-tanzu/platform/tanzu-operations-manager/<%= vars.opsman_version %>/tanzu-ops-manager/pcf-interface.html#internal_auth).
    * If your Tanzu Operations Manager deployment uses SAML authentication:
        1. Click the user account menu in the upper-right corner of the Ops Manager Installation Dashboard.
        1. Click **Settings**.
        1. Select **SAML Settings**.
        1. Activate the **Provision an Admin Client in the BOSH UAA** checkbox.
        1. Click **Enable SAML Authentication**.
        1. Return to the Ops Manager Installation Dashboard.
        1. Click the **BOSH Director** tile.
        1. Select the **Status** tab.
        1. Record the IP address in the **IPs** column of the **BOSH Director** row.
        1. Select the **Credentials** tab.
        1. In the **Uaa Bosh Client Credentials** row of the **BOSH Director** section, click **Link to Credential**.
        1. Record the value of `password`. This value is the secret for **Uaa Bosh Client Credentials**.
        <br>
        <br>
      For more information about SAML authentication settings for your Tanzu Operations Manager deployment, see the [Tanzu Operations Manager documentation](https://techdocs.broadcom.com/us/en/vmware-tanzu/platform/tanzu-operations-manager/<%= vars.opsman_version %>/tanzu-ops-manager/pcf-interface.html#saml).
    * If your Tanzu Operations Manager deployment uses LDAP authentication:
        1. Click the user account menu in the upper-right corner of the Ops Manager Installation Dashboard.
        2. Click **Settings**.
        3. Select **LDAP Settings**.
        4. Activate the **Provision an Admin Client in the BOSH UAA** checkbox.
        5. Click **Enable LDAP Authentication**.
        6. Return to the Ops Manager Installation Dashboard.
        7. Click the **BOSH Director** tile.
        8. Select the **Status** tab.
        9. Record the IP address in the **IPs** column of the **BOSH Director** row.
        10. Select the **Credentials** tab.
        11. In the **Uaa Bosh Client Credentials** row of the **BOSH Director** section, click **Link to Credential**.
        12. Record the value of `password`. This value is the secret for **Uaa Bosh Client Credentials**.
        <br>
        <br>
      For more information about LDAP authentication settings for your Tanzu Operations Manager deployment, see the [Tanzu Operations Manager documentation](https://techdocs.broadcom.com/us/en/vmware-tanzu/platform/tanzu-operations-manager/<%= vars.opsman_version %>/tanzu-ops-manager/pcf-interface.html#ldap).

2. SSH into the Tanzu Operations Manager VM by following the procedure in the [Tanzu Operations Manager documentation](https://techdocs.broadcom.com/us/en/vmware-tanzu/platform/tanzu-operations-manager/<%= vars.opsman_version %>/tanzu-ops-manager/install-trouble-advanced.html#ssh).

3. Target the UAA instance for the BOSH Director by running:

    ```
    uaac target https://BOSH-DIRECTOR-IP:8443 --skip-ssl-validation
    ```
    Where `BOSH-DIRECTOR-IP` is the IP address for the BOSH Director VM that you recorded from the **Status** tab in the BOSH Director tile in a previous
    step.

4. Log in to the UAA instance:
    * If your Tanzu Operations Manager deployment uses internal authentication, log in to the UAA instance by running:

        ```
        uaac token owner get login -s UAA-LOGIN-CLIENT-SECRET
        ```
        Where `UAA-LOGIN-CLIENT-SECRET` is the secret you recorded from the **Uaa Login Client Credentials** row in the **Credentials** tab in the BOSH
        Director tile in a previous step.
    * If your Tanzu Operations Manager deployment uses SAML or LDAP, log in to the UAA instance by running:

        ```
        uaac token client get bosh_admin_client -s BOSH-UAA-CLIENT-SECRET
        ```
        Where `BOSH-UAA-CLIENT-SECRET` is the secret you recorded from the **Uaa Bosh Client Credentials** row in the **Credentials** tab in the BOSH Director
        tile in a previous step.

5. When prompted, enter the UAA administrator client username `admin` and the secret you recorded from the **Uaa Admin Client Credentials** row in the
**Credentials** tab in the BOSH Director tile in a previous step.

1. Create a UAA client for the BOSH deployment metric exporter VM by running:

    ```
    uaac client add CLIENT-USERNAME \
     --secret CLIENT-SECRET \
     --authorized_grant_types client_credentials,refresh_token \
     --authorities bosh.read \
     --scope bosh.read
    ```
    Where:

    * `CLIENT-USERNAME` is the username you want to set for the UAA client.
    * `CLIENT-SECRET` is the secret you want to set for the UAA client.

2. Return to the Ops Manager Installation Dashboard.

3. Click the **Healthwatch Exporter for Tanzu Application Service** tile.

4. Select **BOSH Deployment Metric Exporter VM**.

5. For **UAA client credentials**, enter the username and secret for the UAA client you just created.


## <a id='errands'></a> (Optional) Configure Errands

Errands are scripts that Tanzu Operations Manager runs automatically when it installs or uninstalls a product, such as a new version of Healthwatch Exporter for TAS for
VMs. There are two types of errands: _post-deploy errands_ run after the product is installed, and _pre-delete errands_ run before the product is uninstalled.

By default, Tanzu Operations Manager always runs all errands.

In the **Errands** pane, you can select **On** to always run an errand or **Off** to never run it.

For more information about how Tanzu Operations Manager manages errands, see the [Tanzu Operations Manager documentation](https://techdocs.broadcom.com/us/en/vmware-tanzu/platform/tanzu-operations-manager/<%= vars.opsman_version %>/tanzu-ops-manager/install-managing_errands.html).

To configure the **Errands** pane:

1. Select **Errands**.

1. (Optional) Choose whether to always run or never run the following errands:
    * **Smoke Tests:** Verifies that the metric exporter VMs are running.
    * **Cleanup:** Deletes any existing BOSH deployments created by the BOSH health metric exporter VM for running SLI tests.
    * **Remove CF SLI User:** Deletes the user account that the <%= vars.runtime_prod_full %> SLI exporter VM creates to run the <%= vars.runtime_prod_full %> SLI test suite. For more
    information, see [<%= vars.runtime_prod_full %> SLI Exporter VM](../metrics.html#pas-sli-exporter) in _Healthwatch Metrics_.

1. Click **Save**.


## <a id='syslog'></a> (Optional) Configure Syslog

In the **Syslog** pane, you can configure system logging in Healthwatch Exporter for TAS for VMs to forward log messages from tile component VMs to an
external destination for troubleshooting, such as a remote server or external syslog aggregation service.

To configure the **Syslog** pane:

1. Select **Syslog**.

1. (Optional) Under **Do you want to configure Syslog forwarding?**, select one of the following options:
    * **No, do not forward Syslog:** Disallows syslog forwarding.
    * **Yes:** Allows syslog forwarding and allows you to edit the configuration fields describednbelow.

1. For **Address**, enter the IP address or DNS domain name of your external destination.

1. For **Port**, enter a port on which your external destination listens.

1. For **Transport Protocol**, select **TCP** or **UDP** from the dropdown. This determines which transport protocol Healthwatch Exporter for TAS for VMs uses
to forward system logs to your external destination.

1. (Optional) To transmit logs over TLS:
  1. Activate the **Enable TLS** checkbox. This checkbox is deactivated by default.
  1. For **Permitted Peer**, enter either the name or SHA1 fingerprint of the remote peer.
  1. For **SSL Certificate**, enter the TLS certificate for your external destination.

1. (Optional) For **Queue Size**, specify the number of log messages Healthwatch Exporter for <%= vars.runtime_prod_full %> can hold in a buffer at a time before sending them
to your external destination. The default value is `100000`.

1. (Optional) To forward debug logs to your external destination, activate the **Forward Debug Logs** checkbox. This checkbox is deactivated by default.

1. (Optional) To specify a custom syslog rule, enter it in **Custom rsyslog configuration** in RainerScript syntax. For more information about custom syslog
rules, see the [<%= vars.runtime_prod_full %> documentation](https://techdocs.broadcom.com/us/en/vmware-tanzu/platform/tanzu-platform-for-cloud-foundry/<%= vars.runtime_version %>/tpcf/custom-syslog-rules.html). For more information about
RainerScript syntax, see the [rsyslog documentation](https://www.rsyslog.com/doc/v8-stable/rainerscript/index.html).

1. Click **Save Syslog Settings**.


## <a id='resource-config'></a> (Optional) Configure Resources

In the **Resource Config** pane, you can scale VMs in Healthwatch Exporter for TAS for VMs up or down according to the needs of your deployment, as well as
associate load balancers with a group of VMs. For example, you can scale the persistent disk size of a metric exporter VM to allow longer data retention.

To configure the **Resource Config** pane:

1. Select **Resource Config**.

1. (Optional) To scale a job, select an option from the dropdown for the resource you want to modify:
    * **Instances:** Configures the number of instances each job has.
    * **VM Type:** Configures the type of VM used in each instance.
    * **Persistent Disk Type:** Configures the amount of persistent disk space to allocate to the job.

1. (Optional) To add a load balancer to a job:
  1. Click the icon next to the job name.
  1. For **Load Balancers**, enter the name of your load balancer.
  1. Ensure that the **Internet Connected** checkbox is deactivated. Activating this checkbox gives VMs a public IP address that allows outbound Internet
  access.

1. (Optional) The instance count for the SVM Forwarder VM is set to `0` by default. This VM emits Healthwatch-generated super value metrics (SVMs) into the
Loggregator Firehose. To deploy the SVM Forwarder VM, increase the instance count by selecting from the **Instances** dropdown. You do not need to deploy this
VM unless you use a third-party nozzle that can export the SVMs to an external system, such as a remote server or a syslog aggregation service. For more
information about the SVM Forwarder VM, see [SVM Forwarder VM - Platform Metrics](../metrics.html#svm-forwarder-platform) and [SVM Forwarder VM - Healthwatch
Component Metrics](../metrics.html#svm-forwarder-components) in _Healthwatch Metrics_.

    <p class="note">
    If you installed the Healthwatch Exporter for TAS for VMs tile before installing the Healthwatch tile, you may need
    to re-deploy Healthwatch Exporter for TAS for VMs after deploying the SVM Forwarder VM. For more information, see
    <a href="#apply-changes">Deploy Healthwatch Exporter for TAS for VMs</a> below.</p>

1. (Optional) Healthwatch Exporter for TAS for VMs deploys the counter and gauge metric exporter VMs by default. If you do not want to collect both of these
metric types, set the instance count for the VMs associated with the metrics you do not want to collect to `0`.

1. Click **Save**.


## <a id='apply-changes'></a> Deploy Healthwatch Exporter for TAS for VMs

To complete your installation of the Healthwatch Exporter for TAS for VMs tile:

1. Return to the Ops Manager Installation Dashboard.

1. Click **Review Pending Changes**.

1. Click **Apply Changes**.

For more information, see the [Tanzu Operations Manager documentation](https://techdocs.broadcom.com/us/en/vmware-tanzu/platform/tanzu-operations-manager/<%= vars.opsman_version %>/tanzu-ops-manager/install-review-pending-changes.html).


## <a id='scrape-job-config'></a> Configure a Scrape Job for Healthwatch Exporter for TAS for VMs

After you have successfully deployed Healthwatch Exporter for TAS for VMs, you must configure a scrape job in the Prometheus instance that exists within your
metrics monitoring system. Follow the procedure in one of the following sections, depending on which monitoring system you use:

* If you monitor metrics using the Healthwatch tile on an Tanzu Operations Manager foundation, see [Configure a Scrape Job for Healthwatch Exporter for TAS for VMs in Healthwatch](#scraping-healthwatch) below.

    <p class="note">
    You only need to configure a scrape job for installations of Healthwatch Exporter for TAS for VMs that are not on the
    same Tanzu Operations Manager foundation as your Healthwatch tile. The Prometheus instance in the Healthwatch tile automatically discovers and scrapes Healthwatch
    Exporter tiles that are installed on the same Tanzu Operations Manager foundation as the Healthwatch tile.</p>

* If you monitor metrics using a service or database located outside your Tanzu Operations Manager foundation, such as from an external TSDB, see [Configure a Scrape Job
for Healthwatch Exporter for <%= vars.runtime_prod_full %> in an External Monitoring System](#scraping-external) below.

### <a id='scraping-healthwatch'></a> Configure a Scrape Job for Healthwatch Exporter for <%= vars.runtime_prod_full %> in Healthwatch

To configure a scrape job for Healthwatch Exporter for <%= vars.runtime_prod_full %> in the Healthwatch tile on your Tanzu Operations Manager foundation, see [Configure
Prometheus](configuring-healthwatch.html#prometheus) in _Configuring Healthwatch_.

### <a id='scraping-external'></a> Configure a Scrape Job for Healthwatch Exporter for <%= vars.runtime_prod_full %> in an External Monitoring System

To configure a scrape job for Healthwatch Exporter for <%= vars.runtime_prod_full %> in a service or database that is located outside your Tanzu Operations Manager foundation:

1. Open network communication paths from your external service or database to the metric exporter VMs in Healthwatch Exporter for <%= vars.runtime_prod_full %>. The procedure
to open these network paths differs depending on your Tanzu Operations Manager foundation's IaaS. For a list of TCP ports used by each metric exporter VM, see [Required
Networking Rules for Healthwatch Exporter for <%= vars.runtime_prod_full %>](../architecture.html#network-rules-tas) in _Healthwatch Architecture_.

1. In the `scrape_config` section of the Prometheus configuration file, create a scrape job for your Tanzu Operations Manager foundation. Under `static_config`, specify
the TCP ports of each metric exporter VM as static targets for the IP address of your external service or database. For example:

    ```
    job_name: foundation-1
    metrics_path: /metrics
    scheme: https
    static_configs:
    - targets:
      - "1.2.3.4:8443"
      - "1.2.3.4:25555"
      - "1.2.3.4:443"
      - "1.2.3.4:8082"
    ```
    For more information about the `scrape_config` section of the Prometheus configuration file, see the [Prometheus documentation](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config). For more information about the `static_config`
    section of the Prometheus configuration file, see the [Prometheus documentation](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#static_config).
